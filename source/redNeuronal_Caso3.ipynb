{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CASO 2: Predicción de la demanda de 5 productos top de una empresa\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import sqlalchemy\n",
    "\n",
    "plt.rcParams['figure.figsize' ] = (16, 9)\n",
    "plt.style.use('fast')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_server=\"\"\"\n",
    "DRIVER={ODBC Driver 17 for SQL Server};\n",
    "Server=US3RN4M3;\n",
    "database=demo_prediccion;\n",
    "Trusted_connection=yes;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_top5_productos = \"\"\"\n",
    "SELECT TOP 5\n",
    "[H].[id_DimProducto] AS id,\n",
    "[P].[producto] AS producto\n",
    "FROM [demo_prediccion].[dbo].[hechos] AS [H]\n",
    "INNER JOIN [demo_prediccion].[dbo].[Dim_productos] AS [P] ON [P].[id] = [H].[id_DimProducto]\n",
    "GROUP BY [H].[id_DimProducto], [P].[producto]\n",
    "ORDER BY SUM([H].[cantidad]) DESC, [H].[id_DimProducto] ASC\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#fallando... componer\n",
    "\n",
    "# query = \"\"\"\n",
    "# DECLARE @cols AS NVARCHAR(MAX)\n",
    "# DECLARE @query AS NVARCHAR(MAX)\n",
    "\n",
    "# SET @cols = STUFF((SELECT 'SUM(CASE WHEN [S].[id] = '+ CONVERT(NVARCHAR(10), [SUB]. [id]) + ' THEN [H].[cantidad] ELSE 0 END) AS [' + [SUB].[sucursal] + ']'\n",
    "# FROM(\n",
    "# SELECT\n",
    "# [S].[id] AS id,\n",
    "# [S].[sucursal] AS sucursal\n",
    "# FROM [demo_prediccion].[dbo].[Dim_sucursales] AS [S]\n",
    "# ORDER BY [S].[id] offset 0 rows\n",
    "# ) AS SUB\n",
    "# FOR XML PATH(''), TYPE).value('.', 'NVARCHAR(MAX)'), 1,2,'')\n",
    "\n",
    "# SET @query = '\n",
    "# SELECT\n",
    "# [F].[fecha] AS fecha, ' + @cols + '\n",
    "# FROM\n",
    "# [demo_prediccion]. [dbo]. [hechos] AS [H]\n",
    "# INNER JOIN [demo_prediccion].[dbo].[Dim_fechas] AS [F] ON [F].[id] = [H].[id_DimFechas]\n",
    "# INNER JOIN [demo_prediccion].[dbo].[Dim_productos] AS [P] ON [P].[id] = [H].[id_DimProducto]\n",
    "# INNER JOIN [demo_prediccion].[dbo].[Dim_sucursales] AS [S] ON [S].[id] = [H].[id_DimSucursal]\n",
    "# WHERE\n",
    "# [P].id = ''\"\"\"+'{p0}'+\"\"\"''\n",
    "# GROUP BY \n",
    "# [F].[fecha]\n",
    "# ORDER BY\n",
    "# [F].[fecha] ASC'\n",
    "# \"\"\"\n",
    "# query = \"\"\"\n",
    "# SELECT\n",
    "# [F]. [Fecha] AS fecha,\n",
    "# SUM([H]. [cantidad]) AS cantidad\n",
    "# FROM\n",
    "# [demo_prediccion]. [dbo]. [hechos] AS [H]\n",
    "# INNER JOIN [demo_prediccion]. [dbo]. [Dim_fechas] AS [f] ON [H].[id_DimFechas] = [F].[id]\n",
    "# GROUP BY [F].[Fecha]\n",
    "# \"\"\"\n",
    "\n",
    "#Preelimminar\n",
    "query = \"\"\"\n",
    "DECLARE @cols AS NVARCHAR(MAX);\n",
    "DECLARE @query AS NVARCHAR(MAX);\n",
    "\n",
    "-- Construcción dinámica de las columnas\n",
    "SET @cols = STUFF(\n",
    "    (SELECT ', SUM(CASE WHEN [S].[id] = ' + CONVERT(NVARCHAR(MAX), [SUB].[id]) + \n",
    "            ' THEN [H].[cantidad] ELSE 0 END) AS [' + [SUB].[sucursal] + ']'\n",
    "     FROM (\n",
    "        SELECT [S].[id] AS id, [S].[sucursal] AS sucursal\n",
    "        FROM [demo_prediccion].[dbo].[Dim_sucursales] AS [S]\n",
    "        ORDER BY [S].[id] OFFSET 0 ROWS\n",
    "     ) AS SUB\n",
    "     FOR XML PATH(''), TYPE).value('.', 'NVARCHAR(MAX)'), \n",
    "     1, 2, ''\n",
    ");\n",
    "\n",
    "-- Construcción de la consulta dinámica\n",
    "SET @query = '\n",
    "SELECT\n",
    "    [F].[fecha] AS fecha, ' + @cols + '\n",
    "FROM\n",
    "    [demo_prediccion].[dbo].[hechos] AS [H]\n",
    "    INNER JOIN [demo_prediccion].[dbo].[Dim_fechas] AS [F] ON [F].[id] = [H].[id_DimFechas]\n",
    "    INNER JOIN [demo_prediccion].[dbo].[Dim_productos] AS [P] ON [P].[id] = [H].[id_DimProducto]\n",
    "    INNER JOIN [demo_prediccion].[dbo].[Dim_sucursales] AS [S] ON [S].[id] = [H].[id_DimSucursal]\n",
    "WHERE\n",
    "    [P].[id] = ''\"\"\"+'{p0}'+\"\"\"''\n",
    "GROUP BY \n",
    "    [F].[fecha]\n",
    "ORDER BY\n",
    "    [F].[fecha];';\n",
    "\n",
    "-- Ejecutar la consulta dinámica\n",
    "EXEC sp_executesql @query;\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuración para el entrenamiento (hace referencia a dias)\n",
    "PASOS = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opcion 1: Permitir que pueda ser modificada la sentencia, y mantenrr el try para evitar de que\n",
    "# el programa se rompa\n",
    "\n",
    "def get_sqlconnection(config_sqlServer):\n",
    "    status = \"inicializando....\"\n",
    "    try: \n",
    "        connection = pyodbc.connect(sql_server)\n",
    "        status = \"Conexion establecida satisfactoriamente\"\n",
    "    except Exception as e:\n",
    "        status = \"Error al establecer la conexión:\"+e\n",
    "    print(status)\n",
    "    return connection\n",
    "\n",
    "#Original:\n",
    "# def get_sqlconnection(config_sqlServer):\n",
    "#     connection = pyodbc.connect(config_sqlServer)\n",
    "#     print(\"Se ha conectado a la base de datos\")\n",
    "#     return connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_index_datetime(data):\n",
    "    if str(type(data) == \"<class 'pandas.core.frame.DataFrame'>\"):\n",
    "        # data.sort_values('fecha', inplace=True)\n",
    "        for column in data.columns: \n",
    "            try: \n",
    "                pd.to_datetime(data[column])\n",
    "                data.set_index(column,inplace=True)\n",
    "                return data\n",
    "            except Exception as e:  \n",
    "                pass\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "# def set_index_datetime(data):\n",
    "#     if isinstance(data, pd.DataFrame):\n",
    "#         for column in data.columns:\n",
    "#             try:\n",
    "#                 if column == 'fecha':\n",
    "#                     data[column] = pd.to_datetime(data[column])\n",
    "#                     data.set_index(column, inplace=True)\n",
    "\n",
    "#                 data = data.groupby('fecha').sum().reset_index()\n",
    "#                 data.dropna(inplace=True)\n",
    "#                 return data\n",
    "#                 # data[column] = pd.to_datetime(data[column])\n",
    "#                 # data.set_index(column, inplace=True)\n",
    "#                 # return data\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error al convertir la columna {column} a datetime: {e}\")\n",
    "#                 pass\n",
    "#     else:\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out = 1, dropnan = True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    for i in range(n_in,0,-1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1,i)) for j in range(n_vars)]\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var&d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_y_train(data):\n",
    "    values = data.values\n",
    "    values = values.astype('float32')\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    values= values.reshape(-1, 1)\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    reframed = series_to_supervised(scaled, PASOS, 1)\n",
    "    values = reframed.values\n",
    "    n_train_days = int(len(data)) - (30+PASOS)\n",
    "    train = values[:n_train_days, :]\n",
    "    test = values[n_train_days:, :]\n",
    "    x_train, y_train = train[:, :- 1], train[:, -1]\n",
    "    x_val, y_val = test[:, :- 1], test[:, -1]\n",
    "    x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "    x_val = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "    return x_train, y_train, x_val, y_val, scaler, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_modeloFF():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(PASOS, input_shape=(1,PASOS),activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "    model.compile(loss='mean_absolute_error', optimizer='Adam',metrics=['mse' ])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo(x_train, y_train, x_val, y_val, scaler, values, data, model) :\n",
    "    EPOCHS = 100\n",
    "    model.fit(x_train, y_train, epochs=EPOCHS, validation_data=(x_val, y_val), batch_size=PASOS)\n",
    "    model.predict(x_val)\n",
    "    ultimosDias = data[data.index[int(len(data)*0.70)]:]\n",
    "    values = ultimosDias.values\n",
    "    values = values.astype('float32' )\n",
    "    values = values.reshape(-1, 1)\n",
    "    scaled = values\n",
    "    reframed = series_to_supervised(scaled, PASOS, 1)\n",
    "    reframed.drop(reframed.columns[[12]], axis=1, inplace=True)\n",
    "    values = ultimosDias.values\n",
    "    values = values.astype('float32' )\n",
    "    values = values.reshape(-1, 1)\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    reframed = series_to_supervised(scaled, PASOS, 1)\n",
    "    reframed.drop(reframed.columns[[12]], axis=1, inplace=True)\n",
    "    values = reframed.values\n",
    "    x_test = values[len(values)-1:, :]\n",
    "    x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]) )\n",
    "    return model, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregarNuevoValor(x_test, nuevoValor):\n",
    "    for i in range(x_test.shape[2]-1):\n",
    "        x_test[0][0][i] = x_test[0][0][i+1]\n",
    "    x_test[0][0][x_test. shape[2]-1] = nuevoValor\n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_anomalias(dtaframe):\n",
    "    dataFrame_anomalias = dtaframe.copy()\n",
    "    modeloIsolation = IsolationForest(contamination=0.05)\n",
    "    modeloIsolation.fit(dataFrame_anomalias)\n",
    "    anomalias = modeloIsolation.predict(dataFrame_anomalias)\n",
    "    dtaframe['anomalias' ] = anomalias\n",
    "    dataFrameSinAnomalias = dtaframe[dtaframe['anomalias' ] != -1]\n",
    "    dataFrameSinAnomalias = dataFrameSinAnomalias.drop('anomalias', axis=1)\n",
    "    return dataFrameSinAnomalias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El Core del sistema\n",
    "with get_sqlconnection(sql_server) as cursor:\n",
    "    productos = pd.read_sql_query(query_top5_productos, cursor)\n",
    "    # print(productos)\n",
    "    # productos = set_index_datetime(productos)\n",
    "    table_future_data = []\n",
    "    model = crear_modeloFF()\n",
    "    print(productos)\n",
    "    for id in productos.id:\n",
    "        # print(\"iniciando....\")\n",
    "        cadena = query.format(p0=str(id))\n",
    "        # print(str(id))\n",
    "        # print(cadena)\n",
    "        datos = pd.read_sql_query(cadena, cursor)\n",
    "        datos = set_index_datetime(datos)\n",
    "        last_day = datos.index.max() + timedelta(days=1)\n",
    "        future_days = [last_day + timedelta(days=i) for i in range(PASOS)]\n",
    "        future_data = pd.DataFrame(future_days)\n",
    "        future_data.columns = ['fecha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        for column in datos.columns:\n",
    "            data = datos.filter([column])\n",
    "            data.set_index(datos.index, inplace=True)\n",
    "            data = eliminar_anomalias(data)\n",
    "            x_train, y_train, x_val, y_val, scaler, values = create_x_y_train(data)\n",
    "            model, x_test = entrenar_modelo(x_train, y_train, x_val, y_val, scaler, values, data, model)\n",
    "            results = []\n",
    "            for i in range(PASOS):\n",
    "                parcial = model.predict(x_test)\n",
    "                results.append(parcial[0])\n",
    "                x_test = agregarNuevoValor(x_test, parcial[0])\n",
    "            adimen = [x for x in results]\n",
    "            inverted = scaler.inverse_transform(adimen)\n",
    "            y_pred = pd.DataFrame(inverted.astype(int))\n",
    "            future_data[column]= inverted.astype(int)\n",
    "            del data, x_train, y_train, x_val, y_val, scaler, values, x_test, results, adimen, inverted, y_pred\n",
    "            # future_data = set_index_datetime(future_data)\n",
    "        table_future_data.append(future_data)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # table_future_data\n",
    "pd.options.display.max_columns = None\n",
    "table_future_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envprediccion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
